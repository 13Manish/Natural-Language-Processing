{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Find the Duplicate News Heahlines.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1y2isefZGQ1-CE4UzfeVtJ4wa_s2PcVBv",
      "authorship_tag": "ABX9TyO8gAA5uyzuGHQM+0ujAnRP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manish284/Natural-Language-Processing/blob/master/Find_the_Duplicate_News_Heahlines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbiSFc7BP9kV"
      },
      "source": [
        "Problem Statement:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0owibQYFQF4F"
      },
      "source": [
        "In attached data there are various headlines (Sentences) from various newspapers. \n",
        "Your task is to build a model that can search the similar sentences from the data set.\n",
        "\n",
        "for example: \"where are you currently residing?\" is similar to \"what is your address?\"\n",
        "\n",
        "if you pass query = \" where are you currently residing ?\"\n",
        "Your model should be able to return like something below:\n",
        "\n",
        "1. What is your current address?\n",
        "2. Can you give me your current address?\n",
        "3. Please provide me your address.\n",
        "4. In which city you residing ............... and so on\n",
        "\n",
        "The most similar sentence should be on top and follow the order."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dy4BMMSKeof7"
      },
      "source": [
        "#Importing the required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnGh_Ed2em5Z"
      },
      "source": [
        "import re\n",
        "import os\n",
        "import nltk\n",
        "import gensim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyV2S4Y2jgEI",
        "outputId": "70812343-5890-400c-92fd-ba8cdd6acbe1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#download stopt words\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jVNdN1nSNXQ"
      },
      "source": [
        "# Unzip the data folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwVioOJqQCmo",
        "outputId": "56dc6483-0803-4e09-d263-59b7df8f28b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!unzip /content/news-text.csv.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/news-text.csv.zip\n",
            "  inflating: news-text.csv           \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBUbn3fVe1ia"
      },
      "source": [
        "#Read the data from file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Copucs7KeiHb"
      },
      "source": [
        "dataframe = pd.read_csv('/content/news-text.csv')"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1Eyt-SjfKXR",
        "outputId": "786e4184-3f22-43e1-d504-7d3247c75587",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "#check the top 5 row\n",
        "print(dataframe.head())\n",
        "dataframe.shape"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   publish_date                                      headline_text\n",
            "0      20030219  aba decides against community broadcasting lic...\n",
            "1      20030219     act fire witnesses must be aware of defamation\n",
            "2      20030219     a g calls for infrastructure protection summit\n",
            "3      20030219           air nz staff in aust strike for pay rise\n",
            "4      20030219      air nz strike to affect australian travellers\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1186018, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO-Ul6TPn2CR"
      },
      "source": [
        "# Preprocessing The raw data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMAlX4sEjMaF"
      },
      "source": [
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "GOOD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71qhm1PNjtZE"
      },
      "source": [
        "def text_prepare(text):\n",
        "    text = text.lower()\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
        "    text = GOOD_SYMBOLS_RE.sub('', text)\n",
        "    text = ' '.join([x for x in text.split() if x and x not in STOPWORDS])\n",
        "    return text.strip()"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w591pUibi21p"
      },
      "source": [
        "prepared_dataframe = []\n",
        "for line in dataframe['headline_text']:\n",
        "  prepared_dataframe.append(text_prepare(line))"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGHaScQ1kO66",
        "outputId": "7925cb91-7cd2-47ce-f469-65d715708d9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "#chcking the to 10 row after cleaning\n",
        "prepared_dataframe[0:10]"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aba decides community broadcasting licence',\n",
              " 'act fire witnesses must aware defamation',\n",
              " 'g calls infrastructure protection summit',\n",
              " 'air nz staff aust strike pay rise',\n",
              " 'air nz strike affect australian travellers',\n",
              " 'ambitious olsson wins triple jump',\n",
              " 'antic delighted record breaking barca',\n",
              " 'aussie qualifier stosur wastes four memphis match',\n",
              " 'aust addresses un security council iraq',\n",
              " 'australia locked war timetable opp']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x09iMkOxoNjJ"
      },
      "source": [
        "#Note : -\n",
        "\n",
        "To solve the problem I am going to use two solution\n",
        "\n",
        "1. Pre-train word and Phrase vector\n",
        "    using this method i will prepair the duplicte sentence    present in raw file\n",
        "\n",
        "2. StarSpace\n",
        "    using this model i will train model by raw data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF9d-2NHfdRq"
      },
      "source": [
        "#Downloading Pre-trained word and phrase vectors\n",
        "pre-trained vectors trained on part of Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2tGtHZFfpKZ",
        "outputId": "6996c210-bdb7-4512-b40d-ca97d71973ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# downloading pre-train word2vec\n",
        "!wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-11 20:06:29--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.77.102\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.77.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz.1’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  94.1MB/s    in 42s     \n",
            "\n",
            "2020-10-11 20:07:12 (37.2 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz.1’ saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4xjFA1Igyjh",
        "outputId": "4a2d619c-ad68-49e4-af51-be7336502a3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# uploading word2vec using gensim library\n",
        "wv_embeddings = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True, limit=500000)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:252: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4i-totKhHK7"
      },
      "source": [
        "# hecking Pre-trained word vectors\n",
        "wv_embeddings['me']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6H7wj8ShNTB"
      },
      "source": [
        "# generate the embaddaings of sentence\n",
        "def question_to_vec(question, embeddings, dim=300):\n",
        "    \"\"\"\n",
        "        question: a string\n",
        "        embeddings: dict where the key is a word and a value is its' embedding\n",
        "        dim: size of the representation\n",
        "\n",
        "        result: vector representation for the question\n",
        "    \"\"\"\n",
        "    result = np.zeros(dim)\n",
        "    c = 0\n",
        "    words = question.split()\n",
        "    for word in words:\n",
        "        if word in embeddings:\n",
        "            result += np.array(embeddings[word])\n",
        "            c += 1\n",
        "    if c != 0:\n",
        "        result /= c\n",
        "    return result"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEbKyEVzh1ls"
      },
      "source": [
        "# find out the the cosine semilarties of sentence with other ssentences\n",
        "def rank_candidates(question, candidates, embeddings, dim=300):\n",
        "    \"\"\"\n",
        "        question: a string\n",
        "        candidates: a list of strings (candidates) which we want to rank\n",
        "        embeddings: some embeddings\n",
        "        dim: dimension of the current embeddings\n",
        "        \n",
        "        result: a list of pairs (initial position in the list, question)\n",
        "    \"\"\"\n",
        "    q_vecs = np.array([question_to_vec(question, embeddings, dim) for i in range(len(candidates))])\n",
        "    cand_vecs = np.array([question_to_vec(candidate, embeddings, dim) for candidate in candidates])\n",
        "    cosines = np.array(cosine_similarity(q_vecs, cand_vecs)[0])\n",
        "    merged_list = list(zip(cosines, range(len(candidates)), candidates))\n",
        "    sorted_list  = sorted(merged_list, key=lambda x: x[0], reverse=True)\n",
        "    result = [(a,b,c) for a,b,c in sorted_list if a> 0.4]\n",
        "    return result"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4hb2DG6mnqZ"
      },
      "source": [
        "wv_prepared_ranking = [] \n",
        "for indx,question in enumerate(prepared_dataframe):\n",
        "    candidate=prepared_dataframe\n",
        "    ranks = rank_candidates(question, candidate, wv_embeddings)\n",
        "    wv_prepared_ranking.append([r[2] for r in ranks])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PvIbsGexh3-"
      },
      "source": [
        "# save the similar sentences in single row\n",
        "  out = open('/content/prepared_train.tsv', 'w')\n",
        "  for line in wv_prepared_ranking:\n",
        "    new_line = [text_prepare(q) for q in line]\n",
        "    print(*new_line, sep='\\t', file=out)\n",
        "  out.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1tMj7a6R6fn"
      },
      "source": [
        "prepare_file()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sghn1TMMqZ-O"
      },
      "source": [
        "#2. StarSpace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_M4YVwcqg4I"
      },
      "source": [
        "Traing the the model with the raw data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFG0qGA4VhqG"
      },
      "source": [
        "# Setup\n",
        "def setup_starspace():\n",
        "    if not os.path.exists(\"/usr/local/bin/starspace\"):\n",
        "        os.system(\"wget https://dl.bintray.com/boostorg/release/1.63.0/source/boost_1_63_0.zip\")\n",
        "        os.system(\"unzip boost_1_63_0.zip && mv boost_1_63_0 /usr/local/bin\")\n",
        "        os.system(\"git clone https://github.com/facebookresearch/Starspace.git\")\n",
        "        os.system(\"cd Starspace && make && cp -Rf starspace /usr/local/bin\")\n",
        "setup_starspace()"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEZ0O9HNS70E",
        "outputId": "215db4fe-faed-4668-d336-d6799e697674",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        }
      },
      "source": [
        "# traning the the model with raw data\n",
        "!starspace train -trainFile \"/content/prepared_train.tsv\" -model starspace_embedding \\\n",
        "-trainMode 3 -adagrad true -ngrams 1 -epoch 5 -dim 100 -similarity cosine -minCount 2 \\\n",
        "-verbose true -fileFormat labelDoc -negSearchLimit 10 -lr 0.05"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Arguments: \n",
            "lr: 0.05\n",
            "dim: 100\n",
            "epoch: 5\n",
            "maxTrainTime: 8640000\n",
            "validationPatience: 10\n",
            "saveEveryEpoch: 0\n",
            "loss: hinge\n",
            "margin: 0.05\n",
            "similarity: cosine\n",
            "maxNegSamples: 10\n",
            "negSearchLimit: 10\n",
            "batchSize: 5\n",
            "thread: 10\n",
            "minCount: 2\n",
            "minCountLabel: 1\n",
            "label: __label__\n",
            "label: __label__\n",
            "ngrams: 1\n",
            "bucket: 2000000\n",
            "adagrad: 1\n",
            "trainMode: 3\n",
            "fileFormat: labelDoc\n",
            "normalizeText: 0\n",
            "dropoutLHS: 0\n",
            "dropoutRHS: 0\n",
            "useWeight: 0\n",
            "weightSep: :\n",
            "Start to initialize starspace model.\n",
            "Build dict from input file : /content/prepared_train.tsv\n",
            "\rRead 0M words\n",
            "Number of words in dictionary:  394\n",
            "Number of labels in dictionary: 0\n",
            "Loading data from file : /content/prepared_train.tsv\n",
            "Total number of examples loaded : 89\n",
            "Initialized model weights. Model size :\n",
            "matrix : 394 100\n",
            "Training epoch 0: 0.05 0.01\n",
            "\rEpoch: 88.9%  lr: 0.050000  loss: 0.332594  eta: <1min   tot: 0h0m0s  (17.8%)\n",
            " ---+++                Epoch    0 Train error : 0.61408126 +++--- ☃\n",
            "Training epoch 1: 0.04 0.01\n",
            "\rEpoch: 88.9%  lr: 0.040000  loss: 0.188699  eta: <1min   tot: 0h0m0s  (37.8%)\n",
            " ---+++                Epoch    1 Train error : 0.43622071 +++--- ☃\n",
            "Training epoch 2: 0.03 0.01\n",
            "\rEpoch: 88.9%  lr: 0.030000  loss: 0.498031  eta: <1min   tot: 0h0m0s  (57.8%)\n",
            " ---+++                Epoch    2 Train error : 0.58187366 +++--- ☃\n",
            "Training epoch 3: 0.02 0.01\n",
            "\rEpoch: 88.9%  lr: 0.020000  loss: 0.158896  eta: <1min   tot: 0h0m0s  (77.8%)\n",
            " ---+++                Epoch    3 Train error : 0.30014050 +++--- ☃\n",
            "Training epoch 4: 0.01 0.01\n",
            "Epoch: 88.9%  lr: 0.010000  loss: 0.455309  eta: <1min   tot: 0h0m0s  (97.8%)\n",
            " ---+++                Epoch    4 Train error : 0.34926939 +++--- ☃\n",
            "Saving model to file : starspace_embedding\n",
            "Saving model in tsv format : starspace_embedding.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BLTHSTdWkiO"
      },
      "source": [
        "#storing the generated embaddings\n",
        "starspace_embeddings = dict()\n",
        "for line in open('starspace_embedding.tsv', encoding='utf-8'):\n",
        "    row = line.strip().split('\\t')\n",
        "    starspace_embeddings[row[0]] = np.array(row[1:], dtype=np.float32)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gy928Afogr5U"
      },
      "source": [
        "#checkig the result\n",
        "starspace_ranks_results = []\n",
        "for indx,question in enumerate(prepared_dataframe[0:100]):\n",
        "    candidate=prepared_dataframe[0:100]\n",
        "    ranks = rank_candidates(question, candidate, starspace_embeddings,100)\n",
        "    starspace_ranks_results.append([r[2] for r in ranks])\n"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W6mOmJAh4rI"
      },
      "source": [
        "def get_duplicte(question):\n",
        "  candidate=prepared_dataframe[0:100]\n",
        "  ranks = rank_candidates(question, candidate, starspace_embeddings,100)\n",
        "  print('Question:%s',question)\n",
        "  print('Duplicate Healines:')\n",
        "  print('Rank','\\t\\t\\t','Duplicates')\n",
        "  for i in ranks:\n",
        "    print(i[0],i[2])\n"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0JDfCbvmYSR",
        "outputId": "636fca36-e4bb-4778-b0a3-4fb2ac40caf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "test ='act fire witnesses must aware defamation'\n",
        "get_duplicte(test)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question:%s act fire witnesses must aware defamation\n",
            "Duplicate Healines:\n",
            "Rank \t\t\t Duplicates\n",
            "0.9999999999999997 act fire witnesses must aware defamation\n",
            "0.7779561812346191 businesses prepare terrorist attacks\n",
            "0.5745279379977436 jury consider verdict murder case\n",
            "0.5537601082881319 man charged cooma murder\n",
            "0.44224793381100824 korean subway fire 314 still missing\n",
            "0.42304599351810324 dog mauls 18 month old toddler nsw\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3CrJ51omqUe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}