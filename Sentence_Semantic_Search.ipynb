{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentence Semantic Search.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1y2isefZGQ1-CE4UzfeVtJ4wa_s2PcVBv",
      "authorship_tag": "ABX9TyMEJWdGk4lt12LRWydENvfA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manish284/Natural-Language-Processing/blob/master/Sentence_Semantic_Search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbiSFc7BP9kV"
      },
      "source": [
        "Problem Statement:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0owibQYFQF4F"
      },
      "source": [
        "In attached data there are various headlines (Sentences) from various newspapers. \n",
        "Your task is to build a model that can search the similar sentences from the data set.\n",
        "\n",
        "for example: \"where are you currently residing?\" is similar to \"what is your address?\"\n",
        "\n",
        "if you pass query = \" where are you currently residing ?\"\n",
        "Your model should be able to return like something below:\n",
        "\n",
        "1. What is your current address?\n",
        "2. Can you give me your current address?\n",
        "3. Please provide me your address.\n",
        "4. In which city you residing ............... and so on\n",
        "\n",
        "The most similar sentence should be on top and follow the order."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dy4BMMSKeof7"
      },
      "source": [
        "#importing the required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnGh_Ed2em5Z"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gensim\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyV2S4Y2jgEI",
        "outputId": "70812343-5890-400c-92fd-ba8cdd6acbe1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jVNdN1nSNXQ"
      },
      "source": [
        "## unzip the data folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwVioOJqQCmo",
        "outputId": "56dc6483-0803-4e09-d263-59b7df8f28b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!unzip /content/news-text.csv.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/news-text.csv.zip\n",
            "  inflating: news-text.csv           \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBUbn3fVe1ia"
      },
      "source": [
        "#read the file data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Copucs7KeiHb"
      },
      "source": [
        "dataframe = pd.read_csv('/content/news-text.csv')"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1Eyt-SjfKXR",
        "outputId": "786e4184-3f22-43e1-d504-7d3247c75587",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "#check the top 5 row\n",
        "print(dataframe.head())\n",
        "dataframe.shape"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   publish_date                                      headline_text\n",
            "0      20030219  aba decides against community broadcasting lic...\n",
            "1      20030219     act fire witnesses must be aware of defamation\n",
            "2      20030219     a g calls for infrastructure protection summit\n",
            "3      20030219           air nz staff in aust strike for pay rise\n",
            "4      20030219      air nz strike to affect australian travellers\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1186018, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMAlX4sEjMaF"
      },
      "source": [
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "GOOD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71qhm1PNjtZE"
      },
      "source": [
        "def text_prepare(text):\n",
        "    text = text.lower()\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
        "    text = GOOD_SYMBOLS_RE.sub('', text)\n",
        "    text = ' '.join([x for x in text.split() if x and x not in STOPWORDS])\n",
        "    return text.strip()"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w591pUibi21p"
      },
      "source": [
        "prepared_dataframe = []\n",
        "for line in dataframe['headline_text']:\n",
        "  prepared_dataframe.append(text_prepare(line))"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGHaScQ1kO66",
        "outputId": "7925cb91-7cd2-47ce-f469-65d715708d9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "prepared_dataframe[0:10]"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aba decides community broadcasting licence',\n",
              " 'act fire witnesses must aware defamation',\n",
              " 'g calls infrastructure protection summit',\n",
              " 'air nz staff aust strike pay rise',\n",
              " 'air nz strike affect australian travellers',\n",
              " 'ambitious olsson wins triple jump',\n",
              " 'antic delighted record breaking barca',\n",
              " 'aussie qualifier stosur wastes four memphis match',\n",
              " 'aust addresses un security council iraq',\n",
              " 'australia locked war timetable opp']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF9d-2NHfdRq"
      },
      "source": [
        "#downloading Pre-trained word and phrase vectors\n",
        "pre-trained vectors trained on part of Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2tGtHZFfpKZ",
        "outputId": "6996c210-bdb7-4512-b40d-ca97d71973ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-11 20:06:29--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.77.102\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.77.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz.1’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  94.1MB/s    in 42s     \n",
            "\n",
            "2020-10-11 20:07:12 (37.2 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz.1’ saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4xjFA1Igyjh",
        "outputId": "4a2d619c-ad68-49e4-af51-be7336502a3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "wv_embeddings = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True, limit=500000)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:252: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4i-totKhHK7"
      },
      "source": [
        "# hecking Pre-trained word vectors\n",
        "wv_embeddings['me']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6H7wj8ShNTB"
      },
      "source": [
        "def question_to_vec(question, embeddings, dim=300):\n",
        "    \"\"\"\n",
        "        question: a string\n",
        "        embeddings: dict where the key is a word and a value is its' embedding\n",
        "        dim: size of the representation\n",
        "\n",
        "        result: vector representation for the question\n",
        "    \"\"\"\n",
        "    result = np.zeros(dim)\n",
        "    c = 0\n",
        "    words = question.split()\n",
        "    for word in words:\n",
        "        if word in embeddings:\n",
        "            result += np.array(embeddings[word])\n",
        "            c += 1\n",
        "    if c != 0:\n",
        "        result /= c\n",
        "    return result"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEbKyEVzh1ls"
      },
      "source": [
        "def rank_candidates(question, candidates, embeddings, dim=300):\n",
        "    \"\"\"\n",
        "        question: a string\n",
        "        candidates: a list of strings (candidates) which we want to rank\n",
        "        embeddings: some embeddings\n",
        "        dim: dimension of the current embeddings\n",
        "        \n",
        "        result: a list of pairs (initial position in the list, question)\n",
        "    \"\"\"\n",
        "    q_vecs = np.array([question_to_vec(question, embeddings, dim) for i in range(len(candidates))])\n",
        "    cand_vecs = np.array([question_to_vec(candidate, embeddings, dim) for candidate in candidates])\n",
        "    cosines = np.array(cosine_similarity(q_vecs, cand_vecs)[0])\n",
        "    merged_list = list(zip(cosines, range(len(candidates)), candidates))\n",
        "    sorted_list  = sorted(merged_list, key=lambda x: x[0], reverse=True)\n",
        "    result = [(a,b,c) for a,b,c in sorted_list if a> 0.4]\n",
        "    return result"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4hb2DG6mnqZ"
      },
      "source": [
        "wv_prepared_ranking = [] \n",
        "for indx,question in enumerate(prepared_dataframe[0:100]):\n",
        "    candidate=prepared_dataframe[0:100]\n",
        "    ranks = rank_candidates(question, candidate, wv_embeddings)\n",
        "    wv_prepared_ranking.append([r[2] for r in ranks])"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73SyXCMAqNrj",
        "outputId": "751d40a1-1a96-4da5-aa3a-dc229afda007",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "wv_prepared_ranking[0:2]"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['aba decides community broadcasting licence',\n",
              "  'community urged help homeless youth',\n",
              "  'council chief executive fails secure position',\n",
              "  'council welcomes ambulance levy decision'],\n",
              " ['act fire witnesses must aware defamation',\n",
              "  'bushfire victims urged see centrelink',\n",
              "  'dargo fire threat expected rise',\n",
              "  'jury consider verdict murder case',\n",
              "  'businesses prepare terrorist attacks',\n",
              "  'griffiths fire project knock back',\n",
              "  'brigadier dismisses reports troops harassed',\n",
              "  'firefighters contain acid spill']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PvIbsGexh3-"
      },
      "source": [
        "def prepare_file():\n",
        "  out = open('/content/prepared_train.tsv', 'w')\n",
        "  for line in wv_prepared_ranking:\n",
        "    new_line = [text_prepare(q) for q in line]\n",
        "    print(*new_line, sep='\\t', file=out)\n",
        "  out.close()"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1tMj7a6R6fn"
      },
      "source": [
        "prepare_file()"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFG0qGA4VhqG"
      },
      "source": [
        "def setup_starspace():\n",
        "    if not os.path.exists(\"/usr/local/bin/starspace\"):\n",
        "        os.system(\"wget https://dl.bintray.com/boostorg/release/1.63.0/source/boost_1_63_0.zip\")\n",
        "        os.system(\"unzip boost_1_63_0.zip && mv boost_1_63_0 /usr/local/bin\")\n",
        "        os.system(\"git clone https://github.com/facebookresearch/Starspace.git\")\n",
        "        os.system(\"cd Starspace && make && cp -Rf starspace /usr/local/bin\")"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woT3g691Vjj0"
      },
      "source": [
        "import os\n",
        "setup_starspace()"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEZ0O9HNS70E",
        "outputId": "215db4fe-faed-4668-d336-d6799e697674",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        }
      },
      "source": [
        "!starspace train -trainFile \"/content/prepared_train.tsv\" -model starspace_embedding \\\n",
        "-trainMode 3 -adagrad true -ngrams 1 -epoch 5 -dim 100 -similarity cosine -minCount 2 \\\n",
        "-verbose true -fileFormat labelDoc -negSearchLimit 10 -lr 0.05"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Arguments: \n",
            "lr: 0.05\n",
            "dim: 100\n",
            "epoch: 5\n",
            "maxTrainTime: 8640000\n",
            "validationPatience: 10\n",
            "saveEveryEpoch: 0\n",
            "loss: hinge\n",
            "margin: 0.05\n",
            "similarity: cosine\n",
            "maxNegSamples: 10\n",
            "negSearchLimit: 10\n",
            "batchSize: 5\n",
            "thread: 10\n",
            "minCount: 2\n",
            "minCountLabel: 1\n",
            "label: __label__\n",
            "label: __label__\n",
            "ngrams: 1\n",
            "bucket: 2000000\n",
            "adagrad: 1\n",
            "trainMode: 3\n",
            "fileFormat: labelDoc\n",
            "normalizeText: 0\n",
            "dropoutLHS: 0\n",
            "dropoutRHS: 0\n",
            "useWeight: 0\n",
            "weightSep: :\n",
            "Start to initialize starspace model.\n",
            "Build dict from input file : /content/prepared_train.tsv\n",
            "\rRead 0M words\n",
            "Number of words in dictionary:  394\n",
            "Number of labels in dictionary: 0\n",
            "Loading data from file : /content/prepared_train.tsv\n",
            "Total number of examples loaded : 89\n",
            "Initialized model weights. Model size :\n",
            "matrix : 394 100\n",
            "Training epoch 0: 0.05 0.01\n",
            "\rEpoch: 88.9%  lr: 0.050000  loss: 0.332594  eta: <1min   tot: 0h0m0s  (17.8%)\n",
            " ---+++                Epoch    0 Train error : 0.61408126 +++--- ☃\n",
            "Training epoch 1: 0.04 0.01\n",
            "\rEpoch: 88.9%  lr: 0.040000  loss: 0.188699  eta: <1min   tot: 0h0m0s  (37.8%)\n",
            " ---+++                Epoch    1 Train error : 0.43622071 +++--- ☃\n",
            "Training epoch 2: 0.03 0.01\n",
            "\rEpoch: 88.9%  lr: 0.030000  loss: 0.498031  eta: <1min   tot: 0h0m0s  (57.8%)\n",
            " ---+++                Epoch    2 Train error : 0.58187366 +++--- ☃\n",
            "Training epoch 3: 0.02 0.01\n",
            "\rEpoch: 88.9%  lr: 0.020000  loss: 0.158896  eta: <1min   tot: 0h0m0s  (77.8%)\n",
            " ---+++                Epoch    3 Train error : 0.30014050 +++--- ☃\n",
            "Training epoch 4: 0.01 0.01\n",
            "Epoch: 88.9%  lr: 0.010000  loss: 0.455309  eta: <1min   tot: 0h0m0s  (97.8%)\n",
            " ---+++                Epoch    4 Train error : 0.34926939 +++--- ☃\n",
            "Saving model to file : starspace_embedding\n",
            "Saving model in tsv format : starspace_embedding.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BLTHSTdWkiO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}